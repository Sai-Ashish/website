
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Token-Specific Watermarking</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                 Token-Specific Watermarking with Enhanced Detectability<br> and Semantic Coherence for Large Language Models </br> 
                <small>
                    International Conference on Machine Learning (ICML), 2024</br> 
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://mignonjia.github.io">
                          Mingjia Huo*
                        </a>
                    </li>
                    <li>
                        <a href="https://sai-ashish.github.io/website/">
                          Sai Ashish Somayajula*
                        </a>
                    </li>
                    <li>
                        <a href="https://youweiliang.github.io">
                            Youwei Liang
                        </a>
                    </li>
                    <li>
                        <a href="https://ruisizhang.com/index.html">
                            Ruisi Zhang
                        </a>
                    </li><br>
                    <li>
                        <a href="https://scholar.google.com/citations?user=3XnMVUAAAAAJ&hl=en">
                            Farinaz Koushanfar
                        </a>
                    </li>
                    <li>
                        <a href="https://pengtaoxie.github.io">
                          Pengtao Xie
                        </a>
                    </li>
                </br>University of California, San Diego
                </br>* denotes equal contribution
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://proceedings.mlr.press/v235/huo24a.html">
                            <image src="img/mip_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="Presentation-Poster/Watermark-36X60.pdf">
                            <image src="img/poster-2.png" height="60px">
                                <h4><strong>Poster</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="Presentation-Poster/Watermark_presentation.pdf">
                            <image src="img/ppt.png" height="60px">
                                <h4><strong>Slides</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://www.youtube.com/watch?v=zkLE9ORmrlk">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/mignonjia/TS_watermark">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/ts-w-motivation.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Achieving both the detectability of inserted watermarks and the semantic quality of generated texts is challenging. While current watermarking algorithms have made promising progress in this direction, there remains significant scope for improvement. To address these challenges, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence.
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/zkLE9ORmrlk?si=tr_GIAyHcXmnVVnk" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Introduction
                </h3>
                <p class="text-justify">
                    The rapid advancements in large language models (LLMs) like ChatGPT have revolutionized artificial intelligence (AI), bringing forth unprecedented capabilities and applications. However, this progress has also introduced significant ethical challenges, such as misuse in election manipulation, creation of fake news, and academic dishonesty. Detecting LLM-generated text is crucial to addressing these issues. Traditional classification-based methods are losing effectiveness as LLM-generated texts become indistinguishable from human-written ones, exemplified by OpenAI's AI classifier, which was withdrawn due to low accuracy.
                <!-- <p style="text-align:center;">
                    <image src="img/pe_seq_eqn_pad.png" height="50px" class="img-responsive">
                </p>
                <video id="v0" width="100%" autoplay loop muted>
                  <source src="img/pe_anim_horiz.mp4" type="video/mp4" />
                </video> -->
                <p style="text-align:center;">
                    <image src="img/Generation.png" class="img-responsive" alt="scales" width="100%">
                </p>  
                <p style="text-align:center;">
                    <image src="img/Detection.png" class="img-responsive" alt="scales" width="100%">
                </p>    
                <p class="text-justify">
                    Watermarking techniques have emerged as a promising solution, embedding hidden patterns in LLM-generated texts that algorithms can detect. The KGW method is a notable example, which uses the hash of the preceding token to create a green list and a red list of tokens. A constant value, the watermark logit, is added to the LLM-produced logits on green list tokens, increasing their selection probability. However, the KGW method compromises semantic coherence by using a constant splitting ratio and watermark logit for all tokens, regardless of context. This uniform approach can lead to inappropriate token selection, disrupting the natural flow of text. For instance, with the prefix "The sun rises in the," the contextually appropriate next word is "east." If the splitting ratio is low, fewer tokens are on the green list, making it less likely that "east" will be included in this green list. Further, a high watermark logit increases the probability of selecting from the green list, thus reducing the chance of choosing "east." This rigid method can significantly impair the semantic coherence of the text.
                </p>
                <!-- <p style="text-align:center;">
                    <image src="img/Motivation.png" height="30px" class="img-responsive">
                </p> -->
                <!-- <video id="v0" width="100%" autoplay loop muted>
                  <source src="img/ipe_anim_horiz.mp4" type="video/mp4" />
                </video> -->
                <p class="text-justify">
                    To address this limitation, we introduce a novel approach that simultaneously achieves two primary goals: preserving the semantic integrity of generated texts and ensuring effective watermark detection. Our method dynamically adjusts the splitting ratio and watermark logit for each token during its generation, controlled by two lightweight networks. These networks process the representation of the previous token to determine the optimal splitting ratio and the appropriate watermark logit for the next token. We achieve our objective using two loss functions:

                    1. Watermark detectability: This is quantified via a one-sided z-test, which measures the presence of green tokens in the generated text. Since this metric is non-differentiable, we introduce a differentiable surrogate that allows for direct optimization through gradient-based techniques during training.
                    2. Semantic coherence: This is measured by the cosine similarity between SimCSE embeddings of watermarked and non-watermarked texts.

                    We develop a multi-objective optimization framework that aims to achieve both objectives concurrently, identifying Pareto optimal solutions where improving one objective does not detrimentally affect the other. This balanced approach ensures the effectiveness of watermarking while maintaining the semantic quality of the generated texts.
                </p>
            </div>
        </div>
            


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
                <p class="text-justify">
                    We propose learning token-specific splitting ratio and watermark logit, i.e., ùõæ<sub>t</sub> and ùõø<sub>t</sub>
                </p>
                <p style="text-align:center;">
                    <image src="img/Delta and gamma network.png" class="img-responsive" alt="scales" width="100%">
                </p>                
                <p class="text-justify">
                    We utilize the embeddings of the previous token to generate &gamma;<sub>t</sub> for the current time step, which helps in splitting the vocabulary into red and green list tokens. For each token v in the vocabulary V, y<sub>v</sub><sup>(t)</sup> is sampled from a Bernoulli distribution parameterized by &gamma;<sub>t</sub>. If y<sub>v</sub><sup>(t)</sup> = 1, the token v is assigned to the green list; otherwise, it is assigned to the red list. To make this sampling process differentiable, we apply the Gumbel softmax trick.</p>
                </p>
                <p class="text-justify">
                    Given original logits <i>l<sub>v</sub><sup>(t)</sup></i> for token <i>v</i>, modified logits after biasing the green-list tokens are, l<sub>v</sub><sup>(t)</sup> + y<sub>v</sub><sup>(t)</sup> * &delta;<sub>t</sub>
                </p>
                <p class="text-justify">
                    The following training objectives are used, 
                </p>
                <p class="text-justify">
                    (1) We define a detection loss to improve the detectability. First, we define the z-score function tailored to our token specific ùõæ<sub>t</sub> and ùõø<sub>t</sub>, i.e., 
                </p>
                <p style="text-align:center;">
                    <image src="img/modified_z_score.png" class="img-responsive" alt="scales" width="20%">
                </p> 
                <p class="text-justify"> 
                    Please refer to the paper for more details on the derivation. Since it involves |ùë†|<sub>ùê∫</sub>, which is non-differentiable, we define the following surrogate function. 
                </p>
                <p style="text-align:center;">
                    <image src="img/surrogate_fn.png" class="img-responsive" alt="scales" width="20%">
                </p> 
                <p class="text-justify">
                where ùëù<sub>ùëîùëü</sub><sup>(ùë°)</sup> is the probability of selecting a green token.
                </p>
                <p>Maximize ùëß&#770; or minimize detection loss, ùêø<sub>ùê∑</sub> = ‚àíùëß&#770;.</p>
                <p class="text-justify">
                    (2) Semantic loss to ensure semantic coherence before and after embedding the watermark. Generate sentence embeddings of texts before and after watermarking, i.e., s and s<sub>w</sub>, using the SimCSE model f<sub>&theta;</sub>. Maximize the cosine similarity between them, cos<sub>sim</sub>(f<sub>&theta;</sub>(s), f<sub>&theta;</sub>(s<sub>w</sub>)). Thus, minimize semantic loss, L<sub>S</sub> = ‚àí cos<sub>sim</sub>(f<sub>&theta;</sub>(s), f<sub>&theta;</sub>(s<sub>w</sub>)).                    
                </p>
            </div>
        </div>
            


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <p class="text-justify">
                    The major results of the paper are summarized below. For a more comprehensive explanation, please refer to the paper.
                </p>
                <!-- <p style="text-align:center;">
                    <image src="img/maths.png" class="img-responsive" alt="scales" width="50%">
                </p>                 -->
                <!-- <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/ship_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/chair_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/lego_sbs_path1.mp4" type="video/mp4" />
                </video>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/mic_sbs_path1.mp4" type="video/mp4" />
                </video> -->
                <p class="text-justify">
                    <strong>Comparison of the trade-off for semantic integrity and detectability of different methods applied to OPT-1.3B.</strong>
                </p>        
                <p style="text-align:center;">
                    <img src="img/main results.png" class="img-responsive" alt="scales" style="width:80%; height:auto; display:block; margin-left:auto; margin-right:auto;">
                </p> 
                <!-- <p class="text-justify">
                    We compare our method with Vanilla, CHILD-TUNING<sub>D</sub> , and DPS dense method using BERT<sub>LARGE</sub> across 300, 500, and 1000 training data splits. Reported results are the averaged evaluation metrics over all eight GLUE datasets for each training data split. The highest performance in each row is indicated in <strong>bold</strong>. Our outperforms vanilla and prior FIM-based subnetwork selection based methods by a significant margin.
                </p> -->
                <!-- <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/lego_radii_manip_slider_200p.mp4" type="video/mp4" />
                </video> -->
                <p class="text-justify">
                    <strong>Comparison of EXP-edit and Our Method</strong>
                </p>      
                <p style="text-align:center;">
                    <img src="img/Exp-edit.png" class="img-responsive" alt="scales" style="width:70%; height:auto; display:block; margin-left:auto; margin-right:auto;">
                </p> 
                <!-- <p class="text-justify">
                    Averaged performance across the CoLA, RTE, STSB, and MRPC datasets for Vanilla, Prompt Tuning, Prefix-Tuning, LoRA, and our method in low-resource scenarios with 500 and 1000 training instances. We observe that PEFT methods might not always outperform vanilla finetuning under low-resource scenarios. The primary goal of PEFT methods is to produce comparable performance to vanilla finetuning without incurring huge computational demands. However, our method is guaranteed to produce performance improvements under low-resource scenarios.
                </p> -->
                <p class="text-justify">
                    <strong>Generation and detection speed on OPT-1.3B for generating 200 tokens, measured in seconds.</strong>
                </p>     
                <p style="text-align:center;">
                    <img src="img/Times.png" class="img-responsive" alt="scales" style="width:60%; height:auto; display:block; margin-left:auto; margin-right:auto;">
                </p>                
                <!-- <p class="text-justify">
                    Comparison of our method and vanilla finetuning on five popular PLMs. We evaluated the models using ten runs with different random seeds and reported the results in terms of mean and standard deviation. Average score represents the average performance across four datasets, and the best scores are highlighted in bold. The underlined values indicate occurrences of degenerate seeds. We observe a notable gain over vanilla, along with a substantial decrease in the standard deviation.
                </p> -->
                <p class="text-justify">
                    <strong>Performance of Ours (trained on OPT-1.3B) and KGW when applied to LLAMA2 7B, 13B, and 70B.</strong>
                </p>     
                <p style="text-align:center;">
                    <img src="img/LLAMA.png" class="img-responsive" alt="scales" style="width:60%; height:auto; display:block; margin-left:auto; margin-right:auto;">
                </p>                              
                <!-- <p class="text-justify">
                    Comparison of our method with prior regularization-based methods on four small datasets (CoLA, RTE, MRPC, STSB), known for causing instability in BERT<sub>LARGE</sub>. The mean and standard deviation (std) of ten random seeds are reported for each method. Bold indicates the best performance. Our method surpasses all other baselines in terms of average scores, with a particularly notable improvement on the CoLA dataset over baselines, illustrating the effectiveness of our approach. Double-sided t-tests were performed between our method and the vanilla method. The p-values are less than 0.05, indicating statistically significant performance improvement over vanilla.
                </p>      -->
                <p class="text-justify">
                    <strong>Comparison of our method with KGW under the Dipper paraphrase attack.</strong>
                </p> 
                <p style="text-align:center;">
                    <img src="img/Dipper.png" class="img-responsive" alt="scales" style="width:70%; height:auto; display:block; margin-left:auto; margin-right:auto;">
                </p>   
                <p class="text-justify">
                    <strong>Comparison of our method with KGW under the Copy-Paste-1 (the two figures at the top) and Copy-Paste-3 attack (the two figures at the bottom).</strong>
                </p> 
                <p style="text-align:center;">
                    <img src="img/Copy-paste.png" class="img-responsive" alt="scales" style="width:70%; height:auto; display:block; margin-left:auto; margin-right:auto;">
                </p>  
                <p class="text-justify">
                    <strong> Distribution of watermark logit Œ¥ (left y-axis) and splitting ratio Œ≥ (right y-axis) across different part-of-speech categories of the preceding token.</strong>
                </p> 
                <p style="text-align:center;">
                    <img src="img/delta-gamma-dist.png" class="img-responsive" alt="scales" style="width:70%; height:auto; display:block; margin-left:auto; margin-right:auto;">
                </p>                              
                <!-- <p class="text-justify">
                    Comparison of our method with prior regularization-based methods on four small datasets (CoLA, RTE, MRPC, STSB), known for causing instability in BERT<sub>LARGE</sub>. The mean and standard deviation (std) of ten random seeds are reported for each method. Bold indicates the best performance. Our method surpasses all other baselines in terms of average scores, with a particularly notable improvement on the CoLA dataset over baselines, illustrating the effectiveness of our approach. Double-sided t-tests were performed between our method and the vanilla method. The p-values are less than 0.05, indicating statistically significant performance improvement over vanilla.
                </p> -->
            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Related links
                </h3>
                <p class="text-justify">
                    <a href="https://en.wikipedia.org/wiki/Spatial_anti-aliasing">Wikipedia</a> provides an excellent introduction to spatial anti-aliasing techniques.
                </p>
                <p class="text-justify">
                    Mipmaps were introduced by Lance Williams in his paper "Pyramidal Parametrics" (<a href="https://software.intel.com/sites/default/files/m/7/2/c/p1-williams.pdf">Williams (1983)</a>).
                </p>
                <p class="text-justify">
                    <a href="https://dl.acm.org/doi/abs/10.1145/964965.808589">Amanatides (1984)</a> first proposed the idea of replacing rays with cones in computer graphics rendering. 
                </p>
                <p class="text-justify">
                    The closely related concept of <em>ray differentials</em> (<a href="https://graphics.stanford.edu/papers/trd/">Igehy (1999)</a>) is used in most modern renderers to antialias textures and other material buffers during ray tracing.
                </p>
                <p class="text-justify">
                    Cone tracing has been used along with prefiltered voxel-based representations of scene geometry for speeding up indirect illumination calculations in <a href="https://research.nvidia.com/sites/default/files/publications/GIVoxels-pg2011-authors.pdf">Crassin et al. (2011)</a>.
                </p>
                <p class="text-justify">
                    Attention-Guided-Weights-Mixup was implemented on top of the <a href="https://github.com/google-research/google-research/tree/master/jaxnerf">JAXNeRF</a> codebase.
                </p>
            </div>
        </div> -->
        
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{huo2024token,
title={Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models},
author={Huo, Mingjia and Somayajula, Sai Ashish and Liang, Youwei and Zhang, Ruisi and Koushanfar, Farinaz and Xie, Pengtao},
journal={arXiv preprint arXiv:2402.18059},
year={2024}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <!-- <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We thank Professor Taylor Berg for the insightful discussions on the work. -->
                    <!-- <br>
                    <br> -->
                The website template was borrowed from <a href="http://mgharbi.com/">Micha√´l Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
